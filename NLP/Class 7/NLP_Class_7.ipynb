{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5djCsq83-OTY",
        "outputId": "e6301d95-f544-48c3-fb05-c75db1ad42ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The day is very sunny, also has a soft wind. It is autumn, and I love the session. Unlike summer it is not hard, neither harsh like winter.\""
      ],
      "metadata": {
        "id": "0YVp3LPw-yCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(text)\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "  print(f\"Sentence {i + 1}: {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN_ExkVT_YBV",
        "outputId": "dd9a0a22-c23a-4ceb-e985-83cf8b0a5beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: The day is very sunny, also has a soft wind.\n",
            "Sentence 2: It is autumn, and I love the session.\n",
            "Sentence 3: Unlike summer it is not hard, neither harsh like winter.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Punctuations removing, lowercasing"
      ],
      "metadata": {
        "id": "vqyzIAAs2J72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove punctuations, special characters, url as these creates complexity or noise while training the model\n",
        "\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"Hello! Check out my website: http://example.com. It's awesome! #excited @user $100.\"\n",
        "url_pattern  = r\"http\\S+|www\\S+\"\n",
        "punctuation_pattern = r\"[^a-zA-Z0-9\\s]\"\n",
        "\n",
        "#  clean URL\n",
        "cleaned_text = re.sub(url_pattern, '', text)\n",
        "\n",
        "#  clean punctutations\n",
        "cleaned_text = re.sub(punctuation_pattern, '', text)\n",
        "\n",
        "# lowercase the words\n",
        "cleaned_text = cleaned_text.lower()\n",
        "\n",
        "tokens = word_tokenize(cleaned_text)\n",
        "print(cleaned_text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "AM9gbTl3_54G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5446cb1a-82c9-4d1f-b6fa-97a6420e52dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello check out my website httpexamplecom its awesome excited user 100\n",
            "['hello', 'check', 'out', 'my', 'website', 'httpexamplecom', 'its', 'awesome', 'excited', 'user', '100']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS tagging\n",
        "\n",
        "Parts-Of-Speech tagging"
      ],
      "metadata": {
        "id": "1ymGUSgMWC5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "sentence = \"The world is full of bullshit and toxic people, don't wanna make the place better. Yeah!! I will stop them anyway.\"\n",
        "word_tokens = word_tokenize(sentence)\n",
        "pos_tagged_words = pos_tag(word_tokens)\n",
        "\n",
        "pos_tagged_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7IsbXLP4uly",
        "outputId": "d95eea38-08b4-4356-d31d-a8ef2f9870f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('world', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('full', 'JJ'),\n",
              " ('of', 'IN'),\n",
              " ('bullshit', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('toxic', 'JJ'),\n",
              " ('people', 'NNS'),\n",
              " (',', ','),\n",
              " ('do', 'VBP'),\n",
              " (\"n't\", 'RB'),\n",
              " ('wan', 'VB'),\n",
              " ('na', 'RB'),\n",
              " ('make', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('place', 'NN'),\n",
              " ('better', 'RBR'),\n",
              " ('.', '.'),\n",
              " ('Yeah', 'UH'),\n",
              " ('!', '.'),\n",
              " ('!', '.'),\n",
              " ('I', 'PRP'),\n",
              " ('will', 'MD'),\n",
              " ('stop', 'VB'),\n",
              " ('them', 'PRP'),\n",
              " ('anyway', 'RB'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopwords removal"
      ],
      "metadata": {
        "id": "_ZKbs72rgCgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "sentence = \"The world is full of bullshit and toxic people, don't wanna make the place better. Yeah!! I will stop them anyway.\"\n",
        "sentence = sentence.lower()\n",
        "word_tokens = word_tokenize(sentence)\n",
        "\n",
        "# getting the list of the stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_tokens = [word for word in word_tokens if word not in stop_words]\n",
        "filtered_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H2M0mkSfK-Q",
        "outputId": "c4e0da36-25ee-4090-df93-68fbed792ad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['world',\n",
              " 'full',\n",
              " 'bullshit',\n",
              " 'toxic',\n",
              " 'people',\n",
              " ',',\n",
              " \"n't\",\n",
              " 'wan',\n",
              " 'na',\n",
              " 'make',\n",
              " 'place',\n",
              " 'better',\n",
              " '.',\n",
              " 'yeah',\n",
              " '!',\n",
              " '!',\n",
              " 'stop',\n",
              " 'anyway',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text normalization\n",
        "\n",
        "- To reduce noise by getting rid of irrelevant version of a text."
      ],
      "metadata": {
        "id": "h06TC4i-jlO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text = \"I loveeeeee this! Do you luv it too?\"\n",
        "\n",
        "# Define normalization rules\n",
        "def normalize(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Replace repeated characters (e.g., loveeeeee -> love)\n",
        "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
        "    # Replace slang (e.g., luv -> love)\n",
        "    text = re.sub(r'\\bluv\\b', 'love', text)\n",
        "    return text\n",
        "\n",
        "# Normalize the text\n",
        "normalized_text = normalize(text)\n",
        "\n",
        "# Tokenize the normalized text\n",
        "tokens = word_tokenize(normalized_text)\n",
        "\n",
        "# Print the normalized and tokenized text\n",
        "print(\"Normalized Text:\", normalized_text)\n",
        "print(\"Tokenized Text:\", tokens)"
      ],
      "metadata": {
        "id": "1j1I2onOhXRU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a34cc155-fe53-4b21-bc85-2ef97b6ccb2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Text: i love this! do you love it to?\n",
            "Tokenized Text: ['i', 'love', 'this', '!', 'do', 'you', 'love', 'it', 'to', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ],
      "metadata": {
        "id": "QgeUIqUGKCJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "text = \"I am running in the park and feeling troubled.\"\n",
        "\n",
        "words = word_tokenize(text)\n",
        "stemmer = PorterStemmer()\n",
        "stem_words = [stemmer.stem(word) for word in words]\n",
        "print(stem_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2TQajXmZjYF",
        "outputId": "145a0bed-132c-46a7-a5be-fe52c93e8f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'am', 'run', 'in', 'the', 'park', 'and', 'feel', 'troubl', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization"
      ],
      "metadata": {
        "id": "gY4vkcfbaFgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "text = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "lemmatized_tokens = [word.lemma_ for word in doc]\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTbxM35gdkhq",
        "outputId": "af72af41-a76c-4e5e-8227-0e2fb653c86c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'quick', 'brown', 'fox', 'be', 'jump', 'over', 'the', 'lazy', 'dog', '.']\n"
          ]
        }
      ]
    }
  ]
}