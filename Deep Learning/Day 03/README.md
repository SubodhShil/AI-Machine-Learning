-   Individual errors are called loss whereas, cumulative errors are called cost function.
-   epoch is a parameter refers to the iteration time

## Why activation function is important?

-   Sigmoid and tanh has 'vanishing gradients' problem, which makes the learning process quite slow.
-   Using ReLU instead of sigmoid and tanh it

> Try out various activation function and determine which one fits best for your workflow.

