> **Transfer Learning**: Use the previous learning in the future task. By utilizing this process our model can learn new thing quickly.   
> **Fine Tuning**: Modifying the previous learning according to the current task requirements.



> # **`Pytorch`**

```python
import torch
```

### Create uninitialized tensors with different size

```python
empty_tensor_1D = torch.empty(3)
empty_tensor_1D
```

### Tensor random values

```python
tensor_random_val_1D = torch.rand(3)
tensor_random_val_1D

tensor_random_val_2D = torch.rand(3, 2)
tensor_random_val_2D
```

### Reshape values

```python
x = torch.randn(4, 4)
y = x.view(-1, 8)
z = x.view(-1, 2)
print(x.size(), y.size(), z.size())
```

## **Autograd**

**Gradient descent**: Repetitive process to minimize the cost function using partial derivatives.
