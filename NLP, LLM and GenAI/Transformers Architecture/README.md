## Encoder and Decoder with attention

-   Every word has its own context vector which make it computationally heavy.
-   Processing data sequentially which is not capable for large amounts of data to consume.

## Transformer

-   Removes LSTM/RNN/GRU
-   Self attention

It requires:

1. Huge amount of data.
2. Hardware needs to be powerful.
3. Also requires good amount of time to train.

> Transformer requires a lot of data to generate outcomes. With this much of data it not possible to labeling the output entirely. So, researcher came up with new idea namely **"Unsupervised Language Modeling"**. Finally, this entire large amount of data is called LLM or Large Language Model.

